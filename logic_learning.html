<section id="presentation">
        <div class="container">
                <div class="row">
                        <div class="col-md-12">
<h1>LOGIC AND LEARNING</h1>

<h4>
<p>
The workshop will be held on January 11th and 12th.
</p>

<p>
The programme has four axes, starting from a theoretical standpoint and going to a more practical one: logic and automata, verification, programming languages, and neural networks.
</p>

<p>
There will be 12 invited speakers. Each of the invited speaker either has recently incorporated the learning paradigm into their work, or has proposed some new point of view on an aspect of learning.
</p>

<p>
The programme will not be very dense (about 5h per day) so as to give enough time for discussions.
</p>
</h4>

<h3>Registration is free but mandatory: <a href="https://goo.gl/forms/U6mPdCovzHPOd4cf2">register here</a>!</h3>

                        </div>
                </div>
        </div>
</section>

<section id="programme_thursday_morning">
        <div class="orangeback">
                <div class="container">
                        <div class="row">
                                <div class="col-md-12">
                                <h1>PRELIMINARY PROGRAMME</h1>
                                </div>
                        </div>
                </div>
        </div>
</section>

<section id="thursday_morning">
        <div class="container">
                <div class="row">
                        <div class="col-md-12">
                                <h2>Thursday 11th morning</h2>

<div id="accordion" class="panel-group">

<div class="panel panel-default">

<h3><b>9:30–10:30</b> Logic and automata I</h3>

<div class="panel-heading">
        <h4><b>9:30 - 9:45</b> Nathana&euml;l Fijalkow or David Pym <i>Introduction</i></h4>
</div>

<div class="panel-heading">
        <h4><b>9:45 - 10:30</b> Martin Grohe (Aachen) <i>TBA</i></h4>
        <button type="button" class="btn btn-default" data-toggle="collapse" data-target="#grohe" data-parent="#accordion">((more))</button>
</div>

<div id="grohe" class="panel-collapse collapse">
    <div class="panel-body">

        <dl class="dl-horizontal">
        <dt>Speaker:</dd>
        <dd><a href="https://www.lii.rwth-aachen.de/en/mitarbeiter/13-mitarbeiter/professoren/4-martin-grohe.html">Martin Grohe</a>, RWTH Aachen</dd>

        <dt>Title:</dt>
        <dd>TBA</dd>

        <dt>Abstract:</dt>
        <dd>
        TBA
        </dd>
        </dl>
    </div>
</div>

</div>

<h4>Coffee break</h4>

<div class="panel panel-default">

<h3><b>11:00–12:30</b> Logic and automata II</h3>

<div class="panel-heading">
        <h4><b>11:00 - 11:45</b> Borja Balle (Amazon Research Cambridge) <i>Learning Automata with Hankel Matrices</i></h4>
        <button type="button" class="btn btn-default" data-toggle="collapse" data-target="#balle" data-parent="#accordion">((more))</button>
</div>

<div id="balle" class="panel-collapse collapse">
    <div class="panel-body">

        <dl class="dl-horizontal">
        <dt>Speaker:</dd>
        <dd><a href="http://borjaballe.github.io/">Borja Balle</a>, Amazon Research Cambridge</dd>

        <dt>Title:</dt>
        <dd>Learning Automata with Hankel Matrices</dd>

        <dt>Abstract:</dt>
        <dd>
        The Hankel matrix is a fundamental tool in the theory of weighted automata. In this talk we will describe a general framework for learning automata with Hankel matrices. Our framework provides a unified view of many classical and recent algorithms for learning automata under different learning paradigms, including query learning algorithms, spectral learning algorithms, and Hankel matrix completion algorithms.
        </dd>
        </dl>
    </div>
</div>

<div class="panel-heading">
        <h4><b>11:45 - 12:30</b> Edward Grefenstette (Deepmind) <i>Recurrent Neural Networks and Models of Computation</i></h4>
        <button type="button" class="btn btn-default" data-toggle="collapse" data-target="#grefenstette" data-parent="#accordion">((more))</button>
</div>

<div id="grefenstette" class="panel-collapse collapse">
    <div class="panel-body">

        <dl class="dl-horizontal">
        <dt>Speaker:</dd>
        <dd>Edward Grefenstette, DeepMind</dd>

        <dt>Title:</dt>
        <dd>Recurrent Neural Networks and Models of Computation</dd>

        <dt>Abstract:</dt>
        <dd>
        This talk presents an analysis of various recurrent neural network architectures in terms of traditional models of computation. 
	It makes the case for simpler recurrent architectures being closer to finite state automata, and argues that memory-enhanced architectures support better algorithmic efficiency, 
	even in problems which are describable as regular languages.
        </dd>
        </dl>
    </div>
</div>

</div>

<h4>Lunch</h4>

</div>

                                </div>
                        </div>
                </div>
        </div>
</section>

<section id="programme_thursday_afternoon">
        <div class="orangeback">
                <div class="container">
                        <div class="row">
                                <div class="col-md-12">
                                </div>
                        </div>
                </div>
        </div>
</section>

<section id="thursday_afternoon">
        <div class="container">
                <div class="row">
                        <div class="col-md-12">
                                <h2>Thursday 11th afternoon</h2>

<div id="accordion" class="panel-group">

<div class="panel panel-default">

<h3><b>14:00–15:30</b> Inductive logic programming</h3>

<div class="panel-heading">
        <h4><b>14:00 - 14:45</b> Luc De Raedt (Leuven) <i>Probabilistic logic learning</i></h4>
        <button type="button" class="btn btn-default" data-toggle="collapse" data-target="#deraedt" data-parent="#accordion">((more))</button>
</div>

<div id="deraedt" class="panel-collapse collapse">
    <div class="panel-body">

        <dl class="dl-horizontal">
        <dt>Speaker:</dd>
        <dd><a href="https://people.cs.kuleuven.be/~luc.deraedt/">Luc De Raedt</a>, Leuven</dd>

        <dt>Title:</dt>
        <dd>TBA</dd>

        <dt>Abstract:</dt>
        <dd>
        TBA
        </dd>
        </dl>
    </div>
</div>

<div class="panel-heading">
        <h4><b>14:45 - 15:30</b> Richard Evans (Deepmind) <i>Learning Explanatory Rules from Noisy Data</i></h4>
        <button type="button" class="btn btn-default" data-toggle="collapse" data-target="#evans" data-parent="#accordion">((more))</button>
</div>

<div id="evans" class="panel-collapse collapse">
    <div class="panel-body">

        <dl class="dl-horizontal">
        <dt>Speaker:</dd>
        <dd>Richard Evans</a>, Deepmind</dd>

        <dt>Title:</dt>
        <dd>Learning Explanatory Rules from Noisy Data</dd>

        <dt>Abstract:</dt>
        <dd>
        Artificial Neural Networks are powerful function approximators capable of modelling solutions to a wide variety of problems, both supervised and unsupervised. 
	As their size and expressivity increases, so too does the variance of the model, yielding a nearly ubiquitous overfitting problem. 
	Although mitigated by a variety of model regularisation methods, the common cure is to seek large amounts of training data—which is not necessarily easily obtained—that sufficiently approximates the data distribution of the domain we wish to test on. In contrast, logic programming methods such as Inductive Logic Programming offer an extremely data-efficient process by which models can be trained to reason on symbolic domains. However, these methods are unable to deal with the variety of domains neural networks can be applied to: they are not robust to noise in or mislabelling of inputs, and perhaps more importantly, cannot be applied to non-symbolic domains where the data is ambiguous, such as operating on raw pixels. In this paper, we propose a Differentiable Inductive Logic framework (∂ILP), which can not only solve tasks which traditional ILP systems are suited for, but shows a robustness to noise and error in the training data which ILP cannot cope with. Furthermore, as it is trained by back-propagation against a likelihood objective, it can be hybridised by connecting it with neural networks over ambiguous data in order to be applied to domains which ILP cannot address, while providing data efficiency and generalisation beyond what neural networks on their own can achieve.
        </dd>
        </dl>
    </div>
</div>

</div>

<h4>Coffee break</h4>

<div class="panel panel-default">

<h3><b>16:00–16:45</b> Verification I</h3>

<div class="panel-heading">
        <h4><b>16:00 - 16:45</b> Jane Hillston (University of Edinburgh) <i>Integrating Inference with Stochastic Process Algebra Models</i></h4>
        <button type="button" class="btn btn-default" data-toggle="collapse" data-target="#hillston" data-parent="#accordion">((more))</button>
</div>

<div id="hillston" class="panel-collapse collapse">
    <div class="panel-body">

        <dl class="dl-horizontal">
        <dt>Speaker:</dd>
        <dd><a href="http://homepages.inf.ed.ac.uk/jeh/">Jane Hillston</a>, School of Informatics, University of Edinburgh</dd>

        <dt>Title:</dt>
        <dd>Integrating Inference with Stochastic Process Algebra Models</dd>

        <dt>Abstract:</dt>
        <dd>
        ProPPA is a probabilistic programming language for continuous-time dynamical systems, developed as an extension of the stochastic process algebra Bio-PEPA. It offers a high-level syntax for describing systems of interacting components with stochastic behaviours where some of the parameters are unknown. Such systems occur in many and diverse fields, including biology, ecology and urban transport, and while their modelling and analysis are important, existing methodologies are often not accessible to non-experts, in addition to being tailor-made rather than generally applicable. In particular, parameter learning can be of crucial significance but is a difficult problem due to the complexity of the underlying probabilistic model --- namely, the continuous time setting and the fast-growing number of states.
<br/>
The purpose of the ProPPA framework is to facilitate both the description of these systems and the process of inference, by automating the application of appropriate algorithms. The language is equipped with different parameter inference methods, including a novel MCMC scheme which employs a random truncation strategy to obtain unbiased likelihood estimates. This method is particularly suited to systems with infinite state-spaces, which were previously not manageable without imposing an ad-hoc truncation. Other methods include a naive Approximate Bayesian Computation algorithm and a sampler based on a continuous approximation of the state-space.
        </dd>
        </dl>
    </div>
</div>

</div>

</div>
                        </div>
                </div>
        </div>
</section>

<section id="programme_friday_morning">
        <div class="orangeback">
                <div class="container">
                        <div class="row">
                                <div class="col-md-12">
                                </div>
                        </div>
                </div>
        </div>
</section>

<section id="friday_morning">
        <div class="container">
                <div class="row">
                        <div class="col-md-12">
                                <h2>Friday 12th morning</h2>

<div id="accordion" class="panel-group">

<div class="panel panel-default">

<h3><b>9:00–10:30</b> Verification II</h3>

<div class="panel-heading">
        <h4><b>9:00 - 9:45</b> Jan K&#345;et&iacute;nsk&yacute; (Technical University of Munich) <i>Fast learning of small strategies</i></h4>
        <button type="button" class="btn btn-default" data-toggle="collapse" data-target="#kretinsky" data-parent="#accordion">((more))</button>
</div>

<div id="kretinsky" class="panel-collapse collapse">
    <div class="panel-body">

        <dl class="dl-horizontal">
        <dt>Speaker:</dd>
        <dd><a href="https://www7.in.tum.de/~kretinsk/">Jan K&#345;et&iacute;nsk&yacute;</a>, Technical University of Munich</dd>

        <dt>Title:</dt>
        <dd>Fast learning of small strategies</dd>

        <dt>Abstract:</dt>
        <dd>
        In verification, precise analysis is required, but the algorithms usually suffer from scalability issues. In machine learning, scalability is achieved, but with only very weak guarantees. 
	We show how to merge the two philosophies and profit from both. In this talk, we focus on analysing Markov decision processes. 
	We show how to learn ε-optimal strategies fast and how to represent them concisely so that some understanding of the behaviour and debugging information can be extracted. 
        </dd>
        </dl>
    </div>
</div>


<div class="panel-heading">
        <h4><b>9:45 - 10:30</b> Aditya Nori (Microsoft Cambridge) <i>Learning and fairness</i></h4>
        <button type="button" class="btn btn-default" data-toggle="collapse" data-target="#nori" data-parent="#accordion">((more))</button>
</div>

<div id="nori" class="panel-collapse collapse">
    <div class="panel-body">

        <dl class="dl-horizontal">
        <dt>Speaker:</dd>
        <dd><a href="https://www.microsoft.com/en-us/research/people/adityan/">Aditya Nori</a>, Microsoft Cambridge</dd>

        <dt>Title:</dt>
        <dd>TBA</dd>

        <dt>Abstract:</dt>
        <dd>
        TBA
        </dd>
        </dl>
    </div>
</div>

</div>

<h4>Coffee break</h4>

<div class="panel panel-default">

<h3><b>11:00–12:30</b> Probabilistic programming</h3>

<div class="panel-heading">
        <h4><b>11:00 - 11:45</b> Joost-Pieter Katoen (Aachen) <i>TBA</i></h4>
        <button type="button" class="btn btn-default" data-toggle="collapse" data-target="#katoen" data-parent="#accordion">((more))</button>
</div>

<div id="katoen" class="panel-collapse collapse">
    <div class="panel-body">

        <dl class="dl-horizontal">
        <dt>Speaker:</dd>
        <dd><a href="http://www-i2.informatik.rwth-aachen.de/~katoen/">Joost-Pieter Katoen</a>, RWTH Aachen</dd>

        <dt>Title:</dt>
        <dd>TBA</dd>

        <dt>Abstract:</dt>
        <dd>
        TBA
        </dd>
        </dl>
    </div>
</div>

<div class="panel-heading">
        <h4><b>11:45 - 12:30</b> Sam Staton (Oxford) <i>TBA</i></h4>
        <button type="button" class="btn btn-default" data-toggle="collapse" data-target="#tba2" data-parent="#accordion">((more))</button>
</div>

<div id="tba2" class="panel-collapse collapse">
    <div class="panel-body">

        <dl class="dl-horizontal">
        <dt>Speaker:</dd>
        <dd><a href="">Sam Staton</a>, Oxford</dd>

        <dt>Title:</dt>
        <dd>TBA</dd>

        <dt>Abstract:</dt>
        <dd>
        TBA
        </dd>
        </dl>
    </div>
</div>

</div>

</div>

<h4>Lunch</h4>

                                </div>
                        </div>
                </div>
        </div>
</section>

<section id="programme_friday_afternoon">
        <div class="orangeback">
                <div class="container">
                        <div class="row">
                                <div class="col-md-12">
                                </div>
                        </div>
                </div>
        </div>
</section>

<section id="friday_afternoon">
        <div class="container">
                <div class="row">
                        <div class="col-md-12">
                                <h2>Friday 12th afternoon</h2>

<div id="accordion" class="panel-group">

<div class="panel panel-default">

<h3><b>14:00–15:30</b> Neural networks</h3>

<div class="panel-heading">
        <h4><b>14:00 - 14:45</b> TBA <i>TBA</i></h4>
        <button type="button" class="btn btn-default" data-toggle="collapse" data-target="#tba3" data-parent="#accordion">((more))</button>
</div>

<div id="tba3" class="panel-collapse collapse">
    <div class="panel-body">

        <dl class="dl-horizontal">
        <dt>Speaker:</dd>
        <dd><a href="">TBA</a>, TBA</dd>

        <dt>Title:</dt>
        <dd>TBA</dd>

        <dt>Abstract:</dt>
        <dd>
        TBA
        </dd>
        </dl>
    </div>
</div>

<div class="panel-heading">
        <h4><b>14:45 - 15:30</b> TBA <i>TBA</i></h4>
        <button type="button" class="btn btn-default" data-toggle="collapse" data-target="#tba4" data-parent="#accordion">((more))</button>
</div>

<div id="tba4" class="panel-collapse collapse">
    <div class="panel-body">

        <dl class="dl-horizontal">
        <dt>Speaker:</dd>
        <dd><a href="">TBA</a>, TBA</dd>

        <dt>Title:</dt>
        <dd>TBA</dd>

        <dt>Abstract:</dt>
        <dd>
        TBA
        </dd>
        </dl>
    </div>
</div>

<h4>Coffee break</h4>

<h3><b>16:00–17:00</b> Panel discussion</h3>

<div class="panel-heading">
        <button type="button" class="btn btn-default" data-toggle="collapse" data-target="#panel" data-parent="#accordion">((more))</button>
</div>

<div id="panel" class="panel-collapse collapse">
    <div class="panel-body">

        TBA
    </div>
</div>

</div>

</div>

                        </div>
                </div>
        </div>
</section>


</div>
