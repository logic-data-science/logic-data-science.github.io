<section id="presentation">
        <div class="container">
                <div class="row">
                        <div class="col-md-12">
<h1>LOGIC AND LEARNING</h1>

<h4>
<p>
The workshop will be held on January 11th and 12th.
</p>

<p>
Logic has proved in the last decades a powerful tool in understanding complex systems. It is instrumental in the development of formal methods, which are mathematically based techniques obsessing on hard guarantees. Learning is a pervasive paradigm which has seen tremendous success recently. The use of statistical approaches yields practical solutions to problems which yesterday seemed out of reach.
These two mindsets should not be kept apart, and many efforts have been made recently to combine the formal reasoning offered by logic and the power of learning. 
</p>

<p>
The goal of this workshop is to bring together expertise from various areas to try and understand the opportunities offered by combining logic and learning.
</p>

<p>
The programme has four axes, starting from a theoretical standpoint and going to a more practical one: logic and automata, verification, programming languages, and neural networks.
</p>

<p>
There are 12 invited speakers and a light programme (less than 5h per day) so as to give enough time for discussions.
</p>
</h4>

<h3>Registration is free but mandatory, and seats are limited: <a href="https://goo.gl/forms/U6mPdCovzHPOd4cf2">register here</a>!</h3>

                        </div>
                </div>
        </div>
</section>

<section id="programme_thursday_morning">
        <div class="orangeback">
                <div class="container">
                        <div class="row">
                                <div class="col-md-12">
                                <h1>PROGRAMME (<a href="programme.pdf">PDF version</a>)</h1>
                                </div>
                        </div>
                </div>
        </div>
</section>











<section id="thursday_morning">
        <div class="container">
                <div class="row">
                        <div class="col-md-12">
                                <h2>Thursday 11th morning</h2>

	<div id="accordion" class="panel-group">

		<div class="panel panel-default">

			<h3><b>9:30–10:30</b> Logic and automata I</h3>

			<div class="panel-heading">
			        <h4><b>9:30 - 9:45</b> Nathana&euml;l Fijalkow or David Pym <i>Introduction</i></h4>
			</div>

			<div class="panel-heading">
       				<h4><b>9:45 - 10:30</b> Martin Grohe (RWTH Aachen University) <i>Learning Logically Defined Hypotheses</i></h4>
      				<button type="button" class="btn btn-default" data-toggle="collapse" data-target="#grohe" data-parent="#accordion">((more))</button>
			</div>

			<div id="grohe" class="panel-collapse collapse">
				<div class="panel-body">

			        <dl class="dl-horizontal">
       				<dt>Speaker:</dt>
    				<dd><a href="https://www.lii.rwth-aachen.de/en/mitarbeiter/13-mitarbeiter/professoren/4-martin-grohe.html">Martin Grohe</a>, RWTH Aachen University</dd>

				<dt>Title:</dt>
				<dd>Learning Logically Defined Hypotheses</dd>

				<dt>Abstract:</dt>
				<dd>
I will introduce a declarative framework for machine learning
where hypotheses are defined by formulas of a logic over
some background structure. Within this framework, I will discuss
positive as well as negative learnability results (in the "probably
approximately correct" learning sense) for hypothesis classed defined
in first-order logic and monadic second-order logic over strings,
trees, and graphs of bounded degree. While purely theoretical at this
point, the hope is that our framework may serve as a foundation for
declarative approaches to ML in logic-affine areas such as database
systems or automated verification.
<br/>
(Joint work with Christof Löding and Martin Ritzert.)
				</dd>
				</dl>

				</div>
			</div>

		</div>
	
		<h4>Coffee break</h4>

		<div class="panel panel-default">

			<h3><b>11:00–12:30</b> Logic and automata II</h3>

			<div class="panel-heading">
				<h4><b>11:00 - 11:45</b> Borja Balle (Amazon Research Cambridge) <i>Learning Automata with Hankel Matrices</i></h4>
				<button type="button" class="btn btn-default" data-toggle="collapse" data-target="#balle" data-parent="#accordion">((more))</button>
			</div>

			<div id="balle" class="panel-collapse collapse">
				<div class="panel-body">

				<dl class="dl-horizontal">
				<dt>Speaker:</dt>
				<dd><a href="http://borjaballe.github.io/">Borja Balle</a>, Amazon Research Cambridge</dd>

				<dt>Title:</dt>
				<dd>Learning Automata with Hankel Matrices</dd>

				<dt>Abstract:</dt>
				<dd>
The Hankel matrix is a fundamental tool in the theory of weighted automata. In this talk we will describe a general framework for learning automata with Hankel matrices. Our framework provides a unified view of many classical and recent algorithms for learning automata under different learning paradigms, including query learning algorithms, spectral learning algorithms, and Hankel matrix completion algorithms.
				</dd>
				</dl>

				</div>
			</div>

			<div class="panel-heading">
				<h4><b>11:45 - 12:30</b> Edward Grefenstette (DeepMind) <i>Recurrent Neural Networks and Models of Computation</i></h4>
        			<button type="button" class="btn btn-default" data-toggle="collapse" data-target="#grefenstette" data-parent="#accordion">((more))</button>
			</div>

			<div id="grefenstette" class="panel-collapse collapse">
				<div class="panel-body">

			        <dl class="dl-horizontal">
			        <dt>Speaker:</dt>
			        <dd>Edward Grefenstette, Staff Research Scientist, DeepMind</dd>
				
			        <dt>Title:</dt>
			        <dd>Recurrent Neural Networks and Models of Computation</dd>

       				<dt>Abstract:</dt>
				<dd>
This talk presents an analysis of various recurrent neural network architectures in terms of traditional models of computation. 
It makes the case for simpler recurrent architectures being closer to finite state automata, and argues that memory-enhanced architectures support better algorithmic efficiency, 
even in problems which are describable as regular languages.
			        </dd>

			        </dl>
				</div>
			</div>

		</div>

	</div>

<h4>Lunch</h4>

                        </div>
                </div>
        </div>
</section>








<section id="orange line">
        <div class="orangeback">
                <div class="container">
                        <div class="row">
                                <div class="col-md-12">
				<br/>
                                </div>
                        </div>
                </div>
        </div>
</section>


<section id="thursday_afternoon">
        <div class="container">
                <div class="row">
                        <div class="col-md-12">
                                <h2>Thursday 11th afternoon</h2>

	<div id="accordion" class="panel-group">

		<div class="panel panel-default">

			<h3><b>14:00–15:30</b> Programming languages</h3>

			<div class="panel-heading">
			        <h4><b>14:00 - 14:45</b> Luc De Raedt (Leuven) <i>Probabilistic logic programming and its applications</i></h4>
				<button type="button" class="btn btn-default" data-toggle="collapse" data-target="#deraedt" data-parent="#accordion">((more))</button>
			</div>
	
			<div id="deraedt" class="panel-collapse collapse">
	    			<div class="panel-body">
	        
			        <dl class="dl-horizontal">
			        <dt>Speaker:</dt>
			        <dd><a href="https://people.cs.kuleuven.be/~luc.deraedt/">Luc De Raedt</a>, Leuven</dd>

			        <dt>Title:</dt>
			        <dd>Probabilistic logic programming and its applications</dd>
	
			        <dt>Abstract:</dt>
			        <dd>
Probabilistic programs combine the power of programming languages with that of probabilistic graphical models. There has been a lot of progress in this paradigm over the past twenty years. This talk will introduce probabilistic logic programming languages, which are based on Sato's distribution semantics and which extend probabilistic databases. The key idea is that facts or tuples can be annotated with probabilities that indicate their degree of belief. Together with the rules that encode domain knowledge they induce a set of possible worlds. After an introduction to probabilistic programs, which will cover semantics, inference, and learning, the talk will sketch some emerging applications in knowledge based systems, in cognitive robotics and in answering probability questions.
			        </dd>
			        </dl>
	
				</div>
			</div>
	
			<div class="panel-heading">
			        <h4><b>14:45 - 15:30</b> Aditya Nori (Microsoft Research, Cambridge) <i>Fairness and robustness in machine learning – a formal methods perspective</i></h4>
	        		<button type="button" class="btn btn-default" data-toggle="collapse" data-target="#nori" data-parent="#accordion">((more))</button>
			</div>
	
			<div id="nori" class="panel-collapse collapse">
	    			<div class="panel-body">
	
	        		<dl class="dl-horizontal">
			        <dt>Speaker:</dt>
			        <dd><a href="https://www.microsoft.com/en-us/research/people/adityan/">Aditya Nori</a>, Microsoft Research, Cambridge</dd>
	
			        <dt>Title:</dt>
			        <dd>Fairness and robustness in machine learning – a formal methods perspective</dd>
	
			        <dt>Abstract:</dt>
			        <dd>
With the range and sensitivity of algorithmic decisions expanding at a break-neck speed, it is imperative that
we aggressively investigate fairness and bias in decision-making programs. First, we show that a number of
recently proposed formal definitions of fairness can be encoded as probabilistic program properties. Second,
with the goal of enabling rigorous reasoning about fairness, we design a novel technique for verifying
probabilistic properties that admits a wide class of decision-making programs. Third, we present FairSquare,
the first verification tool for automatically certifying that a program meets a given fairness property. We
evaluate FairSquare on a range of decision-making programs. Our evaluation demonstrates FairSquare's
ability to verify fairness for a range of different programs, which we show are out-of-reach for state-of-the-art
program analysis techniques.
			        </dd>
			        </dl>
	
				</div>
			</div>
		</div>

		<h4>Coffee break</h4>

		<div class="panel panel-default">

			<h3><b>16:00–16:45</b> Probabilistic programming I</h3>

			<div class="panel-heading">
			        <h4><b>16:00 - 16:45</b> Jane Hillston (University of Edinburgh) <i>Integrating Inference with Stochastic Process Algebra Models</i></h4>
			        <button type="button" class="btn btn-default" data-toggle="collapse" data-target="#hillston" data-parent="#accordion">((more))</button>
			</div>
	
			<div id="hillston" class="panel-collapse collapse">
    				<div class="panel-body">
	
			        <dl class="dl-horizontal">
			        <dt>Speaker:</dt>
			        <dd><a href="http://homepages.inf.ed.ac.uk/jeh/">Jane Hillston</a>, School of Informatics, University of Edinburgh</dd>
	
			        <dt>Title:</dt>
			        <dd>Integrating Inference with Stochastic Process Algebra Models</dd>
	
			        <dt>Abstract:</dt>
			        <dd>
ProPPA is a probabilistic programming language for continuous-time dynamical systems, developed as an extension of the stochastic process algebra Bio-PEPA. It offers a high-level syntax for describing systems of interacting components with stochastic behaviours where some of the parameters are unknown. Such systems occur in many and diverse fields, including biology, ecology and urban transport, and while their modelling and analysis are important, existing methodologies are often not accessible to non-experts, in addition to being tailor-made rather than generally applicable. In particular, parameter learning can be of crucial significance but is a difficult problem due to the complexity of the underlying probabilistic model --- namely, the continuous time setting and the fast-growing number of states.
<br/>
The purpose of the ProPPA framework is to facilitate both the description of these systems and the process of inference, by automating the application of appropriate algorithms. The language is equipped with different parameter inference methods, including a novel MCMC scheme which employs a random truncation strategy to obtain unbiased likelihood estimates. This method is particularly suited to systems with infinite state-spaces, which were previously not manageable without imposing an ad-hoc truncation. Other methods include a naive Approximate Bayesian Computation algorithm and a sampler based on a continuous approximation of the state-space.
				</dd>
				</dl>
	
				</div>
			</div>

		</div>

	</div>

                        </div>
                </div>
        </div>
</section>








<section id="orange line">
        <div class="orangeback">
                <div class="container">
                        <div class="row">
                                <div class="col-md-12">
				<br/>
                                </div>
                        </div>
                </div>
        </div>
</section>

<section id="friday_morning">
        <div class="container">
                <div class="row">
                        <div class="col-md-12">
                                <h2>Friday 12th morning</h2>

	<div id="accordion" class="panel-group">

		<div class="panel panel-default">

			<h3><b>9:00–10:30</b> Verification</h3>

			<div class="panel-heading">
			        <h4><b>9:00 - 9:45</b> Jan K&#345;et&iacute;nsk&yacute; (Technical University of Munich) <i>Fast learning of small strategies</i></h4>
			        <button type="button" class="btn btn-default" data-toggle="collapse" data-target="#kretinsky" data-parent="#accordion">((more))</button>
			</div>

			<div id="kretinsky" class="panel-collapse collapse">
				<div class="panel-body">

			        <dl class="dl-horizontal">
			        <dt>Speaker:</dt>
			        <dd><a href="https://www7.in.tum.de/~kretinsk/">Jan K&#345;et&iacute;nsk&yacute;</a>, Technical University of Munich</dd>

			        <dt>Title:</dt>
			        <dd>Fast learning of small strategies</dd>

			        <dt>Abstract:</dt>
			        <dd>
In verification, precise analysis is required, but the algorithms usually suffer from scalability issues. In machine learning, scalability is achieved, but with only very weak guarantees. 
We show how to merge the two philosophies and profit from both. In this talk, we focus on analysing Markov decision processes. 
We show how to learn ε-optimal strategies fast and how to represent them concisely so that some understanding of the behaviour and debugging information can be extracted. 
			        </dd>
			        </dl>

				</div>
			</div>

			<div class="panel-heading">
			        <h4><b>9:45 - 10:30</b> Alessandro Abate (University of Oxford) <i>Formal verification and learning of complex systems</i></h4>
			        <button type="button" class="btn btn-default" data-toggle="collapse" data-target="#abate" data-parent="#accordion">((more))</button>
			</div>

			<div id="abate" class="panel-collapse collapse">
    				<div class="panel-body">

			        <dl class="dl-horizontal">
			        <dt>Speaker:</dt>
			        <dd><a href="https://www.cs.ox.ac.uk/people/alessandro.abate/home.html">Alessandro Abate</a>, University of Oxford</dd>

			        <dt>Title:</dt>
        			<dd>Formal verification and learning of complex systems</dd>

			        <dt>Abstract:</dt>
			        <dd>
Two known shortcomings of standard techniques in formal verification are the limited capability to provide system-level assertions, and the scalability to large-scale, complex models, such as those needed in Cyber-Physical Systems (CPS) applications. This talk covers research which addresses these shortcomings, by bringing model-based and data-driven methods together, which can help pushing the envelope of existing algorithms and tools in formal verification. 
<br/>
In the first part of the talk, I will discuss a new, formal, measurement-driven and model-based automated technique, for the quantitative verification of systems with partly unknown dynamics. I will formulate this setup as a data-driven Bayesian inference problem, formally embedded within a quantitative, model-based verification procedure. I argue that the approach can be applied to complex physical systems (e.g., with spatially continuous variables), driven by external inputs and accessed under noisy measurements. 
<br/>
In the later part of the talk, I will concentrate on learning actions in a model, whilst verifying logical constraints, such as safety requirements. I will introduce a new paradigm called ‘Logically Constrained Reinforcement Learning' to attain this, and explain its virtues and its application over complex systems.   
			        </dd>
			        </dl>

				</div>
			</div>

		</div>

		<h4>Coffee break</h4>

		<div class="panel panel-default">

			<h3><b>11:00–12:30</b> Probabilistic programming II</h3>

			<div class="panel-heading">
			        <h4><b>11:00 - 11:45</b> Joost-Pieter Katoen (RWTH Aachen University) <i>Bayesian Inference by Program Verification</i></h4>
			        <button type="button" class="btn btn-default" data-toggle="collapse" data-target="#katoen" data-parent="#accordion">((more))</button>
			</div>

			<div id="katoen" class="panel-collapse collapse">
				<div class="panel-body">

				<dl class="dl-horizontal">
			        <dt>Speaker:</dt>
        			<dd><a href="http://www-i2.informatik.rwth-aachen.de/~katoen/">Joost-Pieter Katoen</a>, RWTH Aachen University</dd>

        			<dt>Title:</dt>
			        <dd>Bayesian Inference by Program Verification</dd>

			        <dt>Abstract:</dt>
			        <dd>
In this talk, I will give a perspective on inference in Bayes' networks
(BNs) using program verification. I will argue how weakest precondition
reasoning a la Dijkstra can be used for exact inference (and more). As
exact inference is NP-complete, inference is typically done by means of
simulation. I will show how by means of wp-reasoning exact expected
sampling times of BNs can be obtained in a fully automated fashion. An
experimental evaluation on BN benchmarks demonstrates that very large
expected sampling times (in the magnitude of millions of years) can be
inferred within less than a second. This provides a means to decide
whether sampling-based methods are appropriate for a given BN.
			        </dd>
 				</dl>

				</div>
			</div>

			<div class="panel-heading">
        			<h4><b>11:45 - 12:30</b> Sam Staton (Oxford) <i>Denotational validation of higher-order Bayesian inference</i></h4>
        			<button type="button" class="btn btn-default" data-toggle="collapse" data-target="#staton" data-parent="#accordion">((more))</button>
			</div>

			<div id="staton" class="panel-collapse collapse">
				<div class="panel-body">

			        <dl class="dl-horizontal">
			        <dt>Speaker:</dt>
			        <dd><a href="http://www.cs.ox.ac.uk/people/samuel.staton/main.html">Sam Staton</a>, University of Oxford</dd>

			        <dt>Title:</dt>
			        <dd>Denotational validation of higher-order Bayesian inference</dd>

			        <dt>Abstract:</dt>
			        <dd>
We present a modular semantic account of Bayesian inference algorithms for probabilistic programming languages, as used in data science and machine learning. Sophisticated inference algorithms are often explained in terms of composition of smaller parts. However, neither their theoretical justification nor their implementation reflects this modularity. We show how to conceptualise and analyse such inference algorithms as manipulating intermediate representations of probabilistic programs using higher-order functions and inductive types, and their denotational semantics. Semantic accounts of continuous distributions use measurable spaces. However, our use of higher-order functions presents a substantial technical difficulty: it is impossible to define a measurable space structure over the collection of measurable functions between arbitrary measurable spaces that is compatible with standard operations on those functions, such as function application. We overcome this difficulty using quasi-Borel spaces, a recently proposed mathematical structure that supports both function spaces and continuous distributions. We define a class of semantic structures for representing probabilistic programs, and semantic validity criteria for transformations of these representations in terms of distribution preservation. We develop a collection of building blocks for composing representations. We use these building blocks to validate common inference algorithms such as Sequential Monte Carlo and Markov Chain Monte Carlo. To emphasize the connection between the semantic manipulation and its traditional measure theoretic origins, we use Kock's synthetic measure theory. We demonstrate its usefulness by proving a quasi-Borel counterpart to the Metropolis-Hastings-Green theorem.
				</dd>
				</dl>

				</div>
			</div>

		</div>

	</div>

	<h4>Lunch</h4>

                        </div>
                </div>
        </div>
</section>














<section id="orange line">
        <div class="orangeback">
                <div class="container">
                        <div class="row">
                                <div class="col-md-12">
				<br/>
                                </div>
                        </div>
                </div>
        </div>
</section>

<section id="friday_afternoon">
        <div class="container">
                <div class="row">
                        <div class="col-md-12">
                                <h2>Friday 12th afternoon</h2>

	<div id="accordion" class="panel-group">

		<div class="panel panel-default">

			<h3><b>14:00–15:30</b> Neural networks</h3>

			<div class="panel-heading">
			        <h4><b>14:00 - 14:45</b> Richard Evans (DeepMind) <i>Learning Explanatory Rules from Noisy Data</i></h4>
			        <button type="button" class="btn btn-default" data-toggle="collapse" data-target="#evans" data-parent="#accordion">((more))</button>
			</div>

			<div id="evans" class="panel-collapse collapse">
				<div class="panel-body">

			        <dl class="dl-horizontal">
			        <dt>Speaker:</dt>
    			    	<dd>Richard Evans</a>, Senior Research Scientist, DeepMind</dd>

			        <dt>Title:</dt>
			        <dd>Learning Explanatory Rules from Noisy Data</dd>

			        <dt>Abstract:</dt>
			        <dd>
Artificial Neural Networks are powerful function approximators capable of modelling solutions to a wide variety of problems, both supervised and unsupervised. 
As their size and expressivity increases, so too does the variance of the model, yielding a nearly ubiquitous overfitting problem. 
Although mitigated by a variety of model regularisation methods, the common cure is to seek large amounts of training data—which is not necessarily easily obtained—that sufficiently approximates the data distribution of the domain we wish to test on. In contrast, logic programming methods such as Inductive Logic Programming offer an extremely data-efficient process by which models can be trained to reason on symbolic domains. However, these methods are unable to deal with the variety of domains neural networks can be applied to: they are not robust to noise in or mislabelling of inputs, and perhaps more importantly, cannot be applied to non-symbolic domains where the data is ambiguous, such as operating on raw pixels. In this paper, we propose a Differentiable Inductive Logic framework (∂ILP), which can not only solve tasks which traditional ILP systems are suited for, but shows a robustness to noise and error in the training data which ILP cannot cope with. Furthermore, as it is trained by back-propagation against a likelihood objective, it can be hybridised by connecting it with neural networks over ambiguous data in order to be applied to domains which ILP cannot address, while providing data efficiency and generalisation beyond what neural networks on their own can achieve.
				</dd>
				</dl>

				</div>
			</div>

			<div class="panel-heading">
			        <h4><b>14:45 - 15:30</b> Tim Rockt&auml;schel (University of Oxford) <i>End-to-End Differentiable Proving</i></h4>
			        <button type="button" class="btn btn-default" data-toggle="collapse" data-target="#rockt" data-parent="#accordion">((more))</button>
			</div>

			<div id="rockt" class="panel-collapse collapse">
				<div class="panel-body">

			        <dl class="dl-horizontal">
			        <dt>Speaker:</dt>
        			<dd><a href="https://rockt.github.io/">Tim Rockt&auml;schel</a>, University of Oxford</dd>

        			<dt>Title:</dt>
			        <dd>End-to-End Differentiable Proving</dd>

			        <dt>Abstract:</dt>
			        <dd>
We introduce neural networks for end-to-end differentiable proving of queries to knowledge bases by operating on dense vector representations of symbols. These neural networks are constructed recursively by taking inspiration from the backward chaining algorithm as used in Prolog. Specifically, we replace symbolic unification with a differentiable computation on vector representations of symbols using a radial basis function kernel, thereby combining symbolic reasoning with learning subsymbolic vector representations. By using gradient descent, the resulting neural network can be trained to infer facts from a given incomplete knowledge base. It learns to (i) place representations of similar symbols in close proximity in a vector space, (ii) make use of such similarities to prove queries, (iii) induce logical rules, and (iv) use provided and induced logical rules for multi-hop reasoning. We demonstrate that this architecture outperforms ComplEx, a state-of-the-art neural link prediction model, on three out of four benchmark knowledge bases while at the same time inducing interpretable function-free first-order logic rules.
				</dd>
				</dl>

				</div>
			</div>
		</div>
	</div>

	<h4>Coffee break</h4>

	<h3><b>16:00–17:00</b> Concluding remarks and perspectives by <a href="http://www.cs.mcgill.ca/~prakash/">Prakash Panangaden</a> (Mc Gill University) and 
<a href="http://www.cs.ox.ac.uk/people/marta.kwiatkowska/">Marta Kwiatkowska</a> (University of Oxford)</h3>

                        </div>
                </div>
        </div>
</section>

