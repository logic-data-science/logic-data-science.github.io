<section id="presentation">
        <div class="container">
                <div class="row">
                        <div class="col-md-12">
<h1>LOGIC AND LEARNING</h1>

<h4>
<p>
The workshop will be held on January 11th and 12th.
</p>

<p>
The programme has four axes, starting from a theoretical standpoint and going to a more practical one: logic and automata, verification, programming languages, and neural networks.
</p>

<p>
There will be 12 invited speakers. Each of the invited speaker either has recently incorporated the learning paradigm into their work, or has proposed some new point of view on an aspect of learning.
</p>

<p>
The programme will not be very dense (about 5h per day) so as to give enough time for discussions.
</p>
</h4>

<h3>Registration is free but mandatory: <a href="https://goo.gl/forms/U6mPdCovzHPOd4cf2">register here</a>!</h3>

                        </div>
                </div>
        </div>
</section>

<section id="programme_thursday_morning">
        <div class="orangeback">
                <div class="container">
                        <div class="row">
                                <div class="col-md-12">
                                <h1>PRELIMINARY PROGRAMME</h1>
                                </div>
                        </div>
                </div>
        </div>
</section>

<section id="thursday_morning">
        <div class="container">
                <div class="row">
                        <div class="col-md-12">
                                <h2>Thursday 11th morning</h2>

<div id="accordion" class="panel-group">

<div class="panel panel-default">

<h3><b>9:30–10:30</b> Logic and automata I</h3>

<div class="panel-heading">
        <h4><b>9:30 - 9:45</b> Nathana&euml;l Fijalkow or David Pym <i>Introduction</i></h4>
</div>

<div class="panel-heading">
        <h4><b>9:45 - 10:30</b> Martin Grohe (RWTH Aachen University) <i>Learning Logically Defined Hypotheses</i></h4>
        <button type="button" class="btn btn-default" data-toggle="collapse" data-target="#grohe" data-parent="#accordion">((more))</button>
</div>

<div id="grohe" class="panel-collapse collapse">
    <div class="panel-body">

        <dl class="dl-horizontal">
        <dt>Speaker:</dd>
        <dd><a href="https://www.lii.rwth-aachen.de/en/mitarbeiter/13-mitarbeiter/professoren/4-martin-grohe.html">Martin Grohe</a>, RWTH Aachen University</dd>

        <dt>Title:</dt>
        <dd>Learning Logically Defined Hypotheses</dd>

        <dt>Abstract:</dt>
        <dd>
I will introduce a declarative framework for machine learning
where hypotheses are defined by formulas of a logic over
some background structure. Within this framework, I will discuss
positive as well as negative learnability results (in the "probably
approximately correct" learning sense) for hypothesis classed defined
in first-order logic and monadic second-order logic over strings,
trees, and graphs of bounded degree. While purely theoretical at this
point, the hope is that our framework may serve as a foundation for
declarative approaches to ML in logic-affine areas such as database
systems or automated verification.
<br/>
(Joint work with Christof Löding and Martin Ritzert.)
        </dd>
        </dl>
    </div>
</div>

</div>

<h4>Coffee break</h4>

<div class="panel panel-default">

<h3><b>11:00–12:30</b> Logic and automata II</h3>

<div class="panel-heading">
        <h4><b>11:00 - 11:45</b> Borja Balle (Amazon Research Cambridge) <i>Learning Automata with Hankel Matrices</i></h4>
        <button type="button" class="btn btn-default" data-toggle="collapse" data-target="#balle" data-parent="#accordion">((more))</button>
</div>

<div id="balle" class="panel-collapse collapse">
    <div class="panel-body">

        <dl class="dl-horizontal">
        <dt>Speaker:</dd>
        <dd><a href="http://borjaballe.github.io/">Borja Balle</a>, Amazon Research Cambridge</dd>

        <dt>Title:</dt>
        <dd>Learning Automata with Hankel Matrices</dd>

        <dt>Abstract:</dt>
        <dd>
        The Hankel matrix is a fundamental tool in the theory of weighted automata. In this talk we will describe a general framework for learning automata with Hankel matrices. Our framework provides a unified view of many classical and recent algorithms for learning automata under different learning paradigms, including query learning algorithms, spectral learning algorithms, and Hankel matrix completion algorithms.
        </dd>
        </dl>
    </div>
</div>

<div class="panel-heading">
        <h4><b>11:45 - 12:30</b> Edward Grefenstette (DeepMind) <i>Recurrent Neural Networks and Models of Computation</i></h4>
        <button type="button" class="btn btn-default" data-toggle="collapse" data-target="#grefenstette" data-parent="#accordion">((more))</button>
</div>

<div id="grefenstette" class="panel-collapse collapse">
    <div class="panel-body">

        <dl class="dl-horizontal">
        <dt>Speaker:</dd>
        <dd>Edward Grefenstette, Staff Research Scientist, DeepMind</dd>

        <dt>Title:</dt>
        <dd>Recurrent Neural Networks and Models of Computation</dd>

        <dt>Abstract:</dt>
        <dd>
        This talk presents an analysis of various recurrent neural network architectures in terms of traditional models of computation. 
	It makes the case for simpler recurrent architectures being closer to finite state automata, and argues that memory-enhanced architectures support better algorithmic efficiency, 
	even in problems which are describable as regular languages.
        </dd>
        </dl>
    </div>
</div>

</div>

<h4>Lunch</h4>

</div>

                                </div>
                        </div>
                </div>
        </div>
</section>

<section id="programme_thursday_afternoon">
        <div class="orangeback">
                <div class="container">
                        <div class="row">
                                <div class="col-md-12">
                                </div>
                        </div>
                </div>
        </div>
</section>

<section id="thursday_afternoon">
        <div class="container">
                <div class="row">
                        <div class="col-md-12">
                                <h2>Thursday 11th afternoon</h2>

<div id="accordion" class="panel-group">

<div class="panel panel-default">

<h3><b>14:00–15:30</b> Inductive logic programming</h3>

<div class="panel-heading">
        <h4><b>14:00 - 14:45</b> Luc De Raedt (Leuven) <i>Probabilistic logic learning</i></h4>
        <button type="button" class="btn btn-default" data-toggle="collapse" data-target="#deraedt" data-parent="#accordion">((more))</button>
</div>

<div id="deraedt" class="panel-collapse collapse">
    <div class="panel-body">

        <dl class="dl-horizontal">
        <dt>Speaker:</dd>
        <dd><a href="https://people.cs.kuleuven.be/~luc.deraedt/">Luc De Raedt</a>, Leuven</dd>

        <dt>Title:</dt>
        <dd>TBA</dd>

        <dt>Abstract:</dt>
        <dd>
        TBA
        </dd>
        </dl>
    </div>
</div>

<div class="panel-heading">
        <h4><b>14:45 - 15:30</b> Richard Evans (DeepMind) <i>Learning Explanatory Rules from Noisy Data</i></h4>
        <button type="button" class="btn btn-default" data-toggle="collapse" data-target="#evans" data-parent="#accordion">((more))</button>
</div>

<div id="evans" class="panel-collapse collapse">
    <div class="panel-body">

        <dl class="dl-horizontal">
        <dt>Speaker:</dd>
        <dd>Richard Evans</a>, Staff Research Scientist, DeepMind</dd>

        <dt>Title:</dt>
        <dd>Learning Explanatory Rules from Noisy Data</dd>

        <dt>Abstract:</dt>
        <dd>
        Artificial Neural Networks are powerful function approximators capable of modelling solutions to a wide variety of problems, both supervised and unsupervised. 
	As their size and expressivity increases, so too does the variance of the model, yielding a nearly ubiquitous overfitting problem. 
	Although mitigated by a variety of model regularisation methods, the common cure is to seek large amounts of training data—which is not necessarily easily obtained—that sufficiently approximates the data distribution of the domain we wish to test on. In contrast, logic programming methods such as Inductive Logic Programming offer an extremely data-efficient process by which models can be trained to reason on symbolic domains. However, these methods are unable to deal with the variety of domains neural networks can be applied to: they are not robust to noise in or mislabelling of inputs, and perhaps more importantly, cannot be applied to non-symbolic domains where the data is ambiguous, such as operating on raw pixels. In this paper, we propose a Differentiable Inductive Logic framework (∂ILP), which can not only solve tasks which traditional ILP systems are suited for, but shows a robustness to noise and error in the training data which ILP cannot cope with. Furthermore, as it is trained by back-propagation against a likelihood objective, it can be hybridised by connecting it with neural networks over ambiguous data in order to be applied to domains which ILP cannot address, while providing data efficiency and generalisation beyond what neural networks on their own can achieve.
        </dd>
        </dl>
    </div>
</div>

</div>

<h4>Coffee break</h4>

<div class="panel panel-default">

<h3><b>16:00–16:45</b> Verification I</h3>

<div class="panel-heading">
        <h4><b>16:00 - 16:45</b> Jane Hillston (University of Edinburgh) <i>Integrating Inference with Stochastic Process Algebra Models</i></h4>
        <button type="button" class="btn btn-default" data-toggle="collapse" data-target="#hillston" data-parent="#accordion">((more))</button>
</div>

<div id="hillston" class="panel-collapse collapse">
    <div class="panel-body">

        <dl class="dl-horizontal">
        <dt>Speaker:</dd>
        <dd><a href="http://homepages.inf.ed.ac.uk/jeh/">Jane Hillston</a>, School of Informatics, University of Edinburgh</dd>

        <dt>Title:</dt>
        <dd>Integrating Inference with Stochastic Process Algebra Models</dd>

        <dt>Abstract:</dt>
        <dd>
        ProPPA is a probabilistic programming language for continuous-time dynamical systems, developed as an extension of the stochastic process algebra Bio-PEPA. It offers a high-level syntax for describing systems of interacting components with stochastic behaviours where some of the parameters are unknown. Such systems occur in many and diverse fields, including biology, ecology and urban transport, and while their modelling and analysis are important, existing methodologies are often not accessible to non-experts, in addition to being tailor-made rather than generally applicable. In particular, parameter learning can be of crucial significance but is a difficult problem due to the complexity of the underlying probabilistic model --- namely, the continuous time setting and the fast-growing number of states.
<br/>
The purpose of the ProPPA framework is to facilitate both the description of these systems and the process of inference, by automating the application of appropriate algorithms. The language is equipped with different parameter inference methods, including a novel MCMC scheme which employs a random truncation strategy to obtain unbiased likelihood estimates. This method is particularly suited to systems with infinite state-spaces, which were previously not manageable without imposing an ad-hoc truncation. Other methods include a naive Approximate Bayesian Computation algorithm and a sampler based on a continuous approximation of the state-space.
        </dd>
        </dl>
    </div>
</div>

</div>

</div>
                        </div>
                </div>
        </div>
</section>

<section id="programme_friday_morning">
        <div class="orangeback">
                <div class="container">
                        <div class="row">
                                <div class="col-md-12">
                                </div>
                        </div>
                </div>
        </div>
</section>

<section id="friday_morning">
        <div class="container">
                <div class="row">
                        <div class="col-md-12">
                                <h2>Friday 12th morning</h2>

<div id="accordion" class="panel-group">

<div class="panel panel-default">

<h3><b>9:00–10:30</b> Verification II</h3>

<div class="panel-heading">
        <h4><b>9:00 - 9:45</b> Jan K&#345;et&iacute;nsk&yacute; (Technical University of Munich) <i>Fast learning of small strategies</i></h4>
        <button type="button" class="btn btn-default" data-toggle="collapse" data-target="#kretinsky" data-parent="#accordion">((more))</button>
</div>

<div id="kretinsky" class="panel-collapse collapse">
    <div class="panel-body">

        <dl class="dl-horizontal">
        <dt>Speaker:</dd>
        <dd><a href="https://www7.in.tum.de/~kretinsk/">Jan K&#345;et&iacute;nsk&yacute;</a>, Technical University of Munich</dd>

        <dt>Title:</dt>
        <dd>Fast learning of small strategies</dd>

        <dt>Abstract:</dt>
        <dd>
        In verification, precise analysis is required, but the algorithms usually suffer from scalability issues. In machine learning, scalability is achieved, but with only very weak guarantees. 
	We show how to merge the two philosophies and profit from both. In this talk, we focus on analysing Markov decision processes. 
	We show how to learn ε-optimal strategies fast and how to represent them concisely so that some understanding of the behaviour and debugging information can be extracted. 
        </dd>
        </dl>
    </div>
</div>


<div class="panel-heading">
        <h4><b>9:45 - 10:30</b> Aditya Nori (Microsoft Cambridge) <i>Probabilistic Verification of Program Fairness</i></h4>
        <button type="button" class="btn btn-default" data-toggle="collapse" data-target="#nori" data-parent="#accordion">((more))</button>
</div>

<div id="nori" class="panel-collapse collapse">
    <div class="panel-body">

        <dl class="dl-horizontal">
        <dt>Speaker:</dd>
        <dd><a href="https://www.microsoft.com/en-us/research/people/adityan/">Aditya Nori</a>, Microsoft Cambridge</dd>

        <dt>Title:</dt>
        <dd>Probabilistic Verification of Program Fairness</dd>

        <dt>Abstract:</dt>
        <dd>
        With the range and sensitivity of algorithmic decisions expanding at a break-neck speed, it is imperative that
we aggressively investigate fairness and bias in decision-making programs. First, we show that a number of
recently proposed formal definitions of fairness can be encoded as probabilistic program properties. Second,
with the goal of enabling rigorous reasoning about fairness, we design a novel technique for verifying
probabilistic properties that admits a wide class of decision-making programs. Third, we present FairSquare,
the first verification tool for automatically certifying that a program meets a given fairness property. We
evaluate FairSquare on a range of decision-making programs. Our evaluation demonstrates FairSquare’s
ability to verify fairness for a range of different programs, which we show are out-of-reach for state-of-the-art
program analysis techniques.
        </dd>
        </dl>
    </div>
</div>

</div>

<h4>Coffee break</h4>

<div class="panel panel-default">

<h3><b>11:00–12:30</b> Probabilistic programming</h3>

<div class="panel-heading">
        <h4><b>11:00 - 11:45</b> Joost-Pieter Katoen (Aachen) <i>Probabilistic programming</i></h4>
        <button type="button" class="btn btn-default" data-toggle="collapse" data-target="#katoen" data-parent="#accordion">((more))</button>
</div>

<div id="katoen" class="panel-collapse collapse">
    <div class="panel-body">

        <dl class="dl-horizontal">
        <dt>Speaker:</dd>
        <dd><a href="http://www-i2.informatik.rwth-aachen.de/~katoen/">Joost-Pieter Katoen</a>, RWTH Aachen</dd>

        <dt>Title:</dt>
        <dd>Probabilistic programming</dd>

        <dt>Abstract:</dt>
        <dd>
        TBA
        </dd>
        </dl>
    </div>
</div>

<div class="panel-heading">
        <h4><b>11:45 - 12:30</b> Sam Staton (Oxford) <i>Denotational validation of higher-order Bayesian inference</i></h4>
        <button type="button" class="btn btn-default" data-toggle="collapse" data-target="#staton" data-parent="#accordion">((more))</button>
</div>

<div id="staton" class="panel-collapse collapse">
    <div class="panel-body">

        <dl class="dl-horizontal">
        <dt>Speaker:</dd>
        <dd><a href="http://www.cs.ox.ac.uk/people/samuel.staton/main.html">Sam Staton</a>, University of Oxford</dd>

        <dt>Title:</dt>
        <dd>Denotational validation of higher-order Bayesian inference</dd>

        <dt>Abstract:</dt>
        <dd>
        We present a modular semantic account of Bayesian inference algorithms for probabilistic programming languages, as used in data science and machine learning. Sophisticated inference algorithms are often explained in terms of composition of smaller parts. However, neither their theoretical justification nor their implementation reflects this modularity. We show how to conceptualise and analyse such inference algorithms as manipulating intermediate representations of probabilistic programs using higher-order functions and inductive types, and their denotational semantics. Semantic accounts of continuous distributions use measurable spaces. However, our use of higher-order functions presents a substantial technical difficulty: it is impossible to define a measurable space structure over the collection of measurable functions between arbitrary measurable spaces that is compatible with standard operations on those functions, such as function application. We overcome this difficulty using quasi-Borel spaces, a recently proposed mathematical structure that supports both function spaces and continuous distributions. We define a class of semantic structures for representing probabilistic programs, and semantic validity criteria for transformations of these representations in terms of distribution preservation. We develop a collection of building blocks for composing representations. We use these building blocks to validate common inference algorithms such as Sequential Monte Carlo and Markov Chain Monte Carlo. To emphasize the connection between the semantic manipulation and its traditional measure theoretic origins, we use Kock's synthetic measure theory. We demonstrate its usefulness by proving a quasi-Borel counterpart to the Metropolis-Hastings-Green theorem.
        </dd>
        </dl>
    </div>
</div>

</div>

</div>

<h4>Lunch</h4>

                                </div>
                        </div>
                </div>
        </div>
</section>

<section id="programme_friday_afternoon">
        <div class="orangeback">
                <div class="container">
                        <div class="row">
                                <div class="col-md-12">
                                </div>
                        </div>
                </div>
        </div>
</section>

<section id="friday_afternoon">
        <div class="container">
                <div class="row">
                        <div class="col-md-12">
                                <h2>Friday 12th afternoon</h2>

<div id="accordion" class="panel-group">

<div class="panel panel-default">

<h3><b>14:00–15:30</b> Neural networks</h3>

<div class="panel-heading">
        <h4><b>14:00 - 14:45</b> TBA <i>TBA</i></h4>
        <button type="button" class="btn btn-default" data-toggle="collapse" data-target="#tba3" data-parent="#accordion">((more))</button>
</div>

<div id="tba3" class="panel-collapse collapse">
    <div class="panel-body">

        <dl class="dl-horizontal">
        <dt>Speaker:</dd>
        <dd><a href="">TBA</a>, TBA</dd>

        <dt>Title:</dt>
        <dd>TBA</dd>

        <dt>Abstract:</dt>
        <dd>
        TBA
        </dd>
        </dl>
    </div>
</div>

<div class="panel-heading">
        <h4><b>14:45 - 15:30</b> TBA <i>TBA</i></h4>
        <button type="button" class="btn btn-default" data-toggle="collapse" data-target="#tba4" data-parent="#accordion">((more))</button>
</div>

<div id="tba4" class="panel-collapse collapse">
    <div class="panel-body">

        <dl class="dl-horizontal">
        <dt>Speaker:</dd>
        <dd><a href="">TBA</a>, TBA</dd>

        <dt>Title:</dt>
        <dd>TBA</dd>

        <dt>Abstract:</dt>
        <dd>
        TBA
        </dd>
        </dl>
    </div>
</div>

<h4>Coffee break</h4>

<h3><b>16:00–17:00</b> Panel discussion</h3>

<div class="panel-heading">
        <button type="button" class="btn btn-default" data-toggle="collapse" data-target="#panel" data-parent="#accordion">((more))</button>
</div>

<div id="panel" class="panel-collapse collapse">
    <div class="panel-body">

        TBA
    </div>
</div>

</div>

</div>

                        </div>
                </div>
        </div>
</section>


</div>
